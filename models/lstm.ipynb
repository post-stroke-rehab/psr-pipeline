{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c3ec8c",
   "metadata": {},
   "source": [
    "LSTM model architecture. \n",
    "2025-11-01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad0a03",
   "metadata": {},
   "source": [
    "For the architecture:\n",
    "\n",
    "Input x: assuming tensor of shape [B, C, T]\n",
    "- B = batch size\n",
    "- C = number of features per timestep (3 according to Divy)\n",
    "- T = sequence length (time dim)\n",
    "\n",
    "Output x: Tensor of shape [B, 5] \n",
    "- Probabilities for each finger (0-1)\n",
    "\n",
    "Hyperparameters to be tuned, but current hyperparameters should be solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288bd2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=3, hidden_size=64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 5) # 5 output for the fingers\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2) # Transposing input: [B, C, T] â†’ [B, T, C] for LSTM input\n",
    "        x, _ = self.lstm1(x) # Forward pass \n",
    "        x, _ = self.lstm2(x) # Forward pass \n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x) # Sigmoid activation for each finger\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
