{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c37776f",
   "metadata": {},
   "source": [
    "### LSTM model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35e6a8",
   "metadata": {},
   "source": [
    "##### Input x \n",
    "Tensor of shape: [batch, seq_len, num_features=3] \n",
    "- Given 3D input feature vector\n",
    "\n",
    "##### Output x: \n",
    "Tensor of shape: [batch, 5] \n",
    "- Sigmoid probabilities for each finger (0-1)\n",
    "\n",
    "Hyperparameters to be tuned, but current hyperparameters should be solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=3, hidden_size=64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 5)  # 5 output for the fingers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 3)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]  # Keep only last timestep (batch, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)  # Sigmoid activation for each finger\n",
    "        return x  # (batch, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09025b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for training cells\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289941f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=5, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        logits = self.fc(last)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, epochs=5, batch_size=64, lr=1e-3, val_split=0.1, device='cpu'):\n",
    "    \"\"\"Train LSTM on processed data arrays X,y. Prints loss and per-finger accuracy per epoch and returns trained model.\"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import numpy as _np\n",
    "    import torch.nn as _nn\n",
    "\n",
    "    N = len(X)\n",
    "    idx = _np.random.permutation(N)\n",
    "    nval = int(N * val_split)\n",
    "    val_idx = idx[:nval]\n",
    "    train_idx = idx[nval:]\n",
    "\n",
    "    train_ds = ProcessedDataset(X[train_idx], y[train_idx])\n",
    "    val_ds = ProcessedDataset(X[val_idx], y[val_idx])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Prefer user's existing `LSTM_model` if present in the notebook\n",
    "    if 'LSTM_model' in globals():\n",
    "        base_model = LSTM_model()\n",
    "    else:\n",
    "        base_model = LSTMModel(input_size=X.shape[2])\n",
    "\n",
    "    class WrappedModel(torch.nn.Module):\n",
    "        def __init__(self, base, in_dim):\n",
    "            super().__init__()\n",
    "            self.base = base\n",
    "            self.in_dim = in_dim\n",
    "            expected = None\n",
    "            if hasattr(base, 'lstm1') and hasattr(base.lstm1, 'input_size'):\n",
    "                expected = base.lstm1.input_size\n",
    "            elif hasattr(base, 'lstm') and hasattr(base.lstm, 'input_size'):\n",
    "                expected = base.lstm.input_size\n",
    "            self.expected = expected\n",
    "            if expected is not None and expected != in_dim:\n",
    "                self.project = _nn.Linear(in_dim, expected)\n",
    "            else:\n",
    "                self.project = None\n",
    "        def forward(self, x):\n",
    "            if self.project is not None:\n",
    "                x = self.project(x)\n",
    "            return self.base(x)\n",
    "\n",
    "    model = WrappedModel(base_model, X.shape[2]).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Use BCEWithLogitsLoss if model output is unbounded, else BCELoss\n",
    "    with torch.no_grad():\n",
    "        sample = torch.zeros((1, X.shape[1], X.shape[2]), device=device)\n",
    "        out = model(sample)\n",
    "        out_min, out_max = float(out.min()), float(out.max())\n",
    "    if 0.0 <= out_min and out_max <= 1.0:\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        print('Using BCELoss (model returns probabilities).')\n",
    "    else:\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        print('Using BCEWithLogitsLoss (model returns logits).')\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_correct = 0.0\n",
    "        train_total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            # Compute accuracy (threshold 0.5)\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(logits) if not isinstance(loss_fn, torch.nn.BCELoss) else logits\n",
    "                preds = (probs > 0.5).float()\n",
    "                train_correct += (preds == yb).float().sum().item()\n",
    "                train_total += preds.numel()\n",
    "\n",
    "        total_loss /= len(train_ds)\n",
    "        train_acc = train_correct / train_total if train_total > 0 else 0.0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0.0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                logits = model(xb)\n",
    "                val_loss += loss_fn(logits, yb).item() * xb.size(0)\n",
    "                probs = torch.sigmoid(logits) if not isinstance(loss_fn, torch.nn.BCELoss) else logits\n",
    "                preds = (probs > 0.5).float()\n",
    "                val_correct += (preds == yb).float().sum().item()\n",
    "                val_total += preds.numel()\n",
    "        val_loss = val_loss / len(val_ds) if len(val_ds) > 0 else 0.0\n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={total_loss:.4f}, val_loss={val_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d01ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking load_processed availability\n",
    "subjects = list(range(1, 11))\n",
    "try:\n",
    "    X, y = load_processed(subjects)\n",
    "    print('Loaded X,y shapes for example:', X.shape, y.shape)\n",
    "except Exception as e:\n",
    "    print('load_processed not available or failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4123eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dataset saved by data/run_preprocess.py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_processed(subjects, data_dir=Path('..') / 'data' / 'processed'):\n",
    "    \"\"\"Load and concatenate processed X/y for a list of subject ids.\n",
    "\n",
    "    subjects: iterable of ints or strings (e.g. [1,2,3] or ['1','2'])\n",
    "    data_dir: Path to the `data/processed` directory (defaults to ../data/processed from this notebook)\n",
    "    Returns: X (N,T,C), y (N,5)\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for sid in subjects:\n",
    "        s = str(sid)\n",
    "        sdir = data_dir / f\"S{s}\"\n",
    "        Xp = sdir / \"X.npy\"\n",
    "        yp = sdir / \"y.npy\"\n",
    "        if not Xp.exists() or not yp.exists():\n",
    "            raise FileNotFoundError(f\"Missing processed files for S{s} in {sdir}\")\n",
    "        Xs.append(np.load(Xp))\n",
    "        ys.append(np.load(yp))\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    return X, y\n",
    "\n",
    "# Example usage (adjust subject list as needed)\n",
    "subjects = list(range(1, 11))\n",
    "X, y = load_processed(subjects)\n",
    "print('Loaded processed dataset:', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training in-notebook \n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "subjects = list(range(1, 11))\n",
    "X, y = load_processed(subjects)\n",
    "print('Loaded processed data:', X.shape, y.shape)\n",
    "\n",
    "# Run a short training session using the notebook `train_model` which will prefer `LSTM_model` if present\n",
    "model = train_model(X, y, epochs=3, batch_size=128, device=device)\n",
    "\n",
    "# Print a few sample predictions on validation split to inspect outputs\n",
    "import numpy as _np\n",
    "N = len(X)\n",
    "idx = _np.random.permutation(N)\n",
    "nval = int(N * 0.1)\n",
    "val_idx = idx[:nval]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_X = torch.from_numpy(X[val_idx[:8]]).float().to(device)\n",
    "    preds = model(sample_X).cpu().numpy()\n",
    "    print('Sample predictions (first 8):')\n",
    "    print(preds)\n",
    "    print('Sample targets (first 8):')\n",
    "    print(y[val_idx[:8]])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
