{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c3ec8c",
   "metadata": {},
   "source": [
    "### LSTM model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad0a03",
   "metadata": {},
   "source": [
    "##### Input x \n",
    "Tensor of shape: [batch, seq_len, num_features=3] \n",
    "- Given 3D input feature vector\n",
    "\n",
    "##### Output x: \n",
    "Tensor of shape: [batch, 5] \n",
    "- Sigmoid probabilities for each finger (0-1)\n",
    "\n",
    "Hyperparameters to be tuned, but current hyperparameters should be solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288bd2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=3, hidden_size=64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 5) # 5 output for the fingers\n",
    "    def forward(self, x):\n",
    "        # Transpose not needed given input tensor shape\n",
    "        x, _ = self.lstm1(x) \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :] # Keep only last timestep (batch, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x) # Sigmoid activation for each finger\n",
    "        return x # (batch, 5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
